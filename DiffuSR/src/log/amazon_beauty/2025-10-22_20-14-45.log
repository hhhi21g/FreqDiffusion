2025/10/22 20:14:45 - __main__ - INFO - 115 - main - Namespace(dataset='amazon_beauty', log_file='log/', random_seed=1997, max_len=50, device=device(type='cuda', index=0), num_gpu=1, batch_size=512, hidden_size=128, dropout=0.1, emb_dropout=0.3, hidden_act='gelu', num_blocks=4, epochs=500, decay_step=100, gamma=0.1, metric_ks=[5, 10, 20], optimizer='Adam', lr=0.001, loss_lambda=0, weight_decay=0, momentum=None, schedule_sampler_name='lossaware', diffusion_steps=32, lambda_uncertainty=0.001, noise_schedule='trunc_lin', rescale_timesteps=True, eval_interval=20, patience=5, description='Diffu_norm_score', long_head=False, diversity_measure=False, epoch_time_avg=False, model='freqdiffusion', maxlen=200, hidden_units=50, num_epochs=1000, num_heads=1, dropout_rate=0.2, l2_emb=0.0, inference_only=False, state_dict_path=None, norm_first=False)
2025/10/22 20:14:49 - __main__ - INFO - 104 - trainer - Epoch: 0
2025/10/22 20:14:52 - __main__ - INFO - 136 - trainer - [0/257] Loss: 49.3382
2025/10/22 20:15:03 - __main__ - INFO - 136 - trainer - [52/257] Loss: 30.5441
2025/10/22 20:15:15 - __main__ - INFO - 136 - trainer - [104/257] Loss: 25.8031
2025/10/22 20:15:27 - __main__ - INFO - 136 - trainer - [156/257] Loss: 22.8174
2025/10/22 20:15:39 - __main__ - INFO - 136 - trainer - [208/257] Loss: 20.7513
2025/10/22 20:15:49 - __main__ - INFO - 104 - trainer - Epoch: 1
2025/10/22 20:15:49 - __main__ - INFO - 136 - trainer - [0/257] Loss: 18.0699
2025/10/22 20:16:01 - __main__ - INFO - 136 - trainer - [52/257] Loss: 16.6368
2025/10/22 20:16:13 - __main__ - INFO - 136 - trainer - [104/257] Loss: 15.1416
2025/10/22 20:16:25 - __main__ - INFO - 136 - trainer - [156/257] Loss: 13.3747
2025/10/22 20:16:37 - __main__ - INFO - 136 - trainer - [208/257] Loss: 11.5534
2025/10/22 20:16:48 - __main__ - INFO - 104 - trainer - Epoch: 2
2025/10/22 20:16:48 - __main__ - INFO - 136 - trainer - [0/257] Loss: 10.2829
2025/10/22 20:17:00 - __main__ - INFO - 136 - trainer - [52/257] Loss: 10.1017
2025/10/22 20:17:12 - __main__ - INFO - 136 - trainer - [104/257] Loss: 9.9480
2025/10/22 20:17:23 - __main__ - INFO - 136 - trainer - [156/257] Loss: 9.6786
2025/10/22 20:17:35 - __main__ - INFO - 136 - trainer - [208/257] Loss: 9.6510
2025/10/22 20:17:46 - __main__ - INFO - 104 - trainer - Epoch: 3
2025/10/22 20:17:46 - __main__ - INFO - 136 - trainer - [0/257] Loss: 9.5732
2025/10/22 20:17:58 - __main__ - INFO - 136 - trainer - [52/257] Loss: 9.5470
2025/10/22 20:18:10 - __main__ - INFO - 136 - trainer - [104/257] Loss: 9.4804
2025/10/22 20:18:22 - __main__ - INFO - 136 - trainer - [156/257] Loss: 9.4546
2025/10/22 20:18:34 - __main__ - INFO - 136 - trainer - [208/257] Loss: 9.3714
2025/10/22 20:18:45 - __main__ - INFO - 104 - trainer - Epoch: 4
2025/10/22 20:18:45 - __main__ - INFO - 136 - trainer - [0/257] Loss: 9.2273
2025/10/22 20:18:57 - __main__ - INFO - 136 - trainer - [52/257] Loss: 9.2205
2025/10/22 20:19:08 - __main__ - INFO - 136 - trainer - [104/257] Loss: 9.2720
2025/10/22 20:19:20 - __main__ - INFO - 136 - trainer - [156/257] Loss: 9.0962
2025/10/22 20:19:31 - __main__ - INFO - 136 - trainer - [208/257] Loss: 9.0673
2025/10/22 20:19:42 - __main__ - INFO - 104 - trainer - Epoch: 5
2025/10/22 20:19:42 - __main__ - INFO - 136 - trainer - [0/257] Loss: 9.1736
2025/10/22 20:19:53 - __main__ - INFO - 136 - trainer - [52/257] Loss: 9.0411
2025/10/22 20:20:05 - __main__ - INFO - 136 - trainer - [104/257] Loss: 9.0196
2025/10/22 20:20:16 - __main__ - INFO - 136 - trainer - [156/257] Loss: 9.0992
2025/10/22 20:20:27 - __main__ - INFO - 136 - trainer - [208/257] Loss: 8.9967
2025/10/22 20:20:38 - __main__ - INFO - 104 - trainer - Epoch: 6
2025/10/22 20:20:39 - __main__ - INFO - 136 - trainer - [0/257] Loss: 8.9222
2025/10/22 20:20:50 - __main__ - INFO - 136 - trainer - [52/257] Loss: 8.7610
2025/10/22 20:21:02 - __main__ - INFO - 136 - trainer - [104/257] Loss: 8.9407
2025/10/22 20:21:13 - __main__ - INFO - 136 - trainer - [156/257] Loss: 8.8551
