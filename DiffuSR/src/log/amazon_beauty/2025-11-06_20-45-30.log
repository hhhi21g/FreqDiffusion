2025/11/06 20:45:30 - __main__ - INFO - 115 - main - Namespace(dataset='amazon_beauty', log_file='log/', random_seed=1997, max_len=50, device=device(type='cuda', index=0), num_gpu=1, batch_size=512, hidden_size=128, dropout=0.1, emb_dropout=0.3, hidden_act='gelu', num_blocks=4, epochs=500, decay_step=100, gamma=0.1, metric_ks=[5, 10, 20], optimizer='Adam', lr=0.001, loss_lambda=0, weight_decay=0, momentum=None, schedule_sampler_name='lossaware', diffusion_steps=32, lambda_uncertainty=0.001, noise_schedule='trunc_lin', rescale_timesteps=True, eval_interval=20, patience=5, description='Diffu_norm_score', long_head=False, diversity_measure=False, epoch_time_avg=False, model='freqdiffusion', maxlen=200, hidden_units=50, num_epochs=1000, num_heads=1, dropout_rate=0.2, l2_emb=0.0, inference_only=False, state_dict_path=None, norm_first=False)
2025/11/06 20:45:32 - __main__ - INFO - 104 - trainer - Epoch: 0
2025/11/06 20:45:33 - __main__ - INFO - 139 - trainer - [0/257] Loss: 48.1678
2025/11/06 20:45:35 - __main__ - INFO - 139 - trainer - [52/257] Loss: 30.2157
2025/11/06 20:45:38 - __main__ - INFO - 139 - trainer - [104/257] Loss: 25.6126
2025/11/06 20:45:41 - __main__ - INFO - 139 - trainer - [156/257] Loss: 22.2117
2025/11/06 20:45:44 - __main__ - INFO - 139 - trainer - [208/257] Loss: 20.0105
2025/11/06 20:45:46 - __main__ - INFO - 104 - trainer - Epoch: 1
2025/11/06 20:45:46 - __main__ - INFO - 139 - trainer - [0/257] Loss: 18.1178
2025/11/06 20:45:49 - __main__ - INFO - 139 - trainer - [52/257] Loss: 16.2848
2025/11/06 20:45:52 - __main__ - INFO - 139 - trainer - [104/257] Loss: 14.9192
2025/11/06 20:45:55 - __main__ - INFO - 139 - trainer - [156/257] Loss: 13.4687
2025/11/06 20:45:58 - __main__ - INFO - 139 - trainer - [208/257] Loss: 11.6914
2025/11/06 20:46:00 - __main__ - INFO - 104 - trainer - Epoch: 2
2025/11/06 20:46:00 - __main__ - INFO - 139 - trainer - [0/257] Loss: 10.9745
2025/11/06 20:46:03 - __main__ - INFO - 139 - trainer - [52/257] Loss: 10.0629
2025/11/06 20:46:06 - __main__ - INFO - 139 - trainer - [104/257] Loss: 9.8722
2025/11/06 20:46:09 - __main__ - INFO - 139 - trainer - [156/257] Loss: 9.7856
2025/11/06 20:46:12 - __main__ - INFO - 139 - trainer - [208/257] Loss: 9.6947
2025/11/06 20:46:14 - __main__ - INFO - 104 - trainer - Epoch: 3
2025/11/06 20:46:14 - __main__ - INFO - 139 - trainer - [0/257] Loss: 9.6700
2025/11/06 20:46:17 - __main__ - INFO - 139 - trainer - [52/257] Loss: 9.5484
2025/11/06 20:46:20 - __main__ - INFO - 139 - trainer - [104/257] Loss: 9.5200
2025/11/06 20:46:23 - __main__ - INFO - 139 - trainer - [156/257] Loss: 9.3777
2025/11/06 20:46:26 - __main__ - INFO - 139 - trainer - [208/257] Loss: 9.4464
2025/11/06 20:46:28 - __main__ - INFO - 104 - trainer - Epoch: 4
2025/11/06 20:46:28 - __main__ - INFO - 139 - trainer - [0/257] Loss: 9.3272
2025/11/06 20:46:31 - __main__ - INFO - 139 - trainer - [52/257] Loss: 9.3069
2025/11/06 20:46:34 - __main__ - INFO - 139 - trainer - [104/257] Loss: 9.2587
2025/11/06 20:46:37 - __main__ - INFO - 139 - trainer - [156/257] Loss: 9.0967
2025/11/06 20:46:39 - __main__ - INFO - 139 - trainer - [208/257] Loss: 9.0346
2025/11/06 20:46:42 - __main__ - INFO - 104 - trainer - Epoch: 5
2025/11/06 20:46:42 - __main__ - INFO - 139 - trainer - [0/257] Loss: 9.0134
2025/11/06 20:46:45 - __main__ - INFO - 139 - trainer - [52/257] Loss: 9.0501
2025/11/06 20:46:48 - __main__ - INFO - 139 - trainer - [104/257] Loss: 9.0290
2025/11/06 20:46:50 - __main__ - INFO - 139 - trainer - [156/257] Loss: 9.0791
2025/11/06 20:46:53 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.9384
2025/11/06 20:46:56 - __main__ - INFO - 104 - trainer - Epoch: 6
2025/11/06 20:46:56 - __main__ - INFO - 139 - trainer - [0/257] Loss: 8.8732
2025/11/06 20:46:59 - __main__ - INFO - 139 - trainer - [52/257] Loss: 8.8498
2025/11/06 20:47:02 - __main__ - INFO - 139 - trainer - [104/257] Loss: 8.7745
2025/11/06 20:47:04 - __main__ - INFO - 139 - trainer - [156/257] Loss: 8.8996
2025/11/06 20:47:07 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.7776
2025/11/06 20:47:10 - __main__ - INFO - 104 - trainer - Epoch: 7
2025/11/06 20:47:10 - __main__ - INFO - 139 - trainer - [0/257] Loss: 8.6920
2025/11/06 20:47:13 - __main__ - INFO - 139 - trainer - [52/257] Loss: 8.7913
2025/11/06 20:47:15 - __main__ - INFO - 139 - trainer - [104/257] Loss: 8.7192
2025/11/06 20:47:18 - __main__ - INFO - 139 - trainer - [156/257] Loss: 8.6911
2025/11/06 20:47:21 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.6902
2025/11/06 20:47:23 - __main__ - INFO - 104 - trainer - Epoch: 8
2025/11/06 20:47:24 - __main__ - INFO - 139 - trainer - [0/257] Loss: 8.5800
2025/11/06 20:47:26 - __main__ - INFO - 139 - trainer - [52/257] Loss: 8.6894
2025/11/06 20:47:29 - __main__ - INFO - 139 - trainer - [104/257] Loss: 8.6669
2025/11/06 20:47:32 - __main__ - INFO - 139 - trainer - [156/257] Loss: 8.6216
2025/11/06 20:47:35 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.6587
2025/11/06 20:47:37 - __main__ - INFO - 104 - trainer - Epoch: 9
2025/11/06 20:47:37 - __main__ - INFO - 139 - trainer - [0/257] Loss: 8.5279
2025/11/06 20:47:40 - __main__ - INFO - 139 - trainer - [52/257] Loss: 8.6148
2025/11/06 20:47:43 - __main__ - INFO - 139 - trainer - [104/257] Loss: 8.5913
2025/11/06 20:47:46 - __main__ - INFO - 139 - trainer - [156/257] Loss: 8.5995
2025/11/06 20:47:48 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.4999
2025/11/06 20:47:51 - __main__ - INFO - 104 - trainer - Epoch: 10
2025/11/06 20:47:51 - __main__ - INFO - 139 - trainer - [0/257] Loss: 8.4226
2025/11/06 20:47:54 - __main__ - INFO - 139 - trainer - [52/257] Loss: 8.5466
2025/11/06 20:47:57 - __main__ - INFO - 139 - trainer - [104/257] Loss: 8.5353
2025/11/06 20:47:59 - __main__ - INFO - 139 - trainer - [156/257] Loss: 8.5102
2025/11/06 20:48:02 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.5110
2025/11/06 20:48:05 - __main__ - INFO - 104 - trainer - Epoch: 11
2025/11/06 20:48:05 - __main__ - INFO - 139 - trainer - [0/257] Loss: 8.6576
2025/11/06 20:48:08 - __main__ - INFO - 139 - trainer - [52/257] Loss: 8.3787
2025/11/06 20:48:11 - __main__ - INFO - 139 - trainer - [104/257] Loss: 8.3353
2025/11/06 20:48:13 - __main__ - INFO - 139 - trainer - [156/257] Loss: 8.2423
2025/11/06 20:48:16 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.3287
2025/11/06 20:48:19 - __main__ - INFO - 104 - trainer - Epoch: 12
2025/11/06 20:48:19 - __main__ - INFO - 139 - trainer - [0/257] Loss: 8.1865
2025/11/06 20:48:22 - __main__ - INFO - 139 - trainer - [52/257] Loss: 8.4728
2025/11/06 20:48:25 - __main__ - INFO - 139 - trainer - [104/257] Loss: 8.3122
2025/11/06 20:48:27 - __main__ - INFO - 139 - trainer - [156/257] Loss: 8.2768
2025/11/06 20:48:30 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.4209
2025/11/06 20:48:33 - __main__ - INFO - 104 - trainer - Epoch: 13
2025/11/06 20:48:33 - __main__ - INFO - 139 - trainer - [0/257] Loss: 8.2951
2025/11/06 20:48:36 - __main__ - INFO - 139 - trainer - [52/257] Loss: 8.3015
2025/11/06 20:48:38 - __main__ - INFO - 139 - trainer - [104/257] Loss: 8.3083
2025/11/06 20:48:41 - __main__ - INFO - 139 - trainer - [156/257] Loss: 8.2666
2025/11/06 20:48:44 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.2324
2025/11/06 20:48:47 - __main__ - INFO - 104 - trainer - Epoch: 14
2025/11/06 20:48:47 - __main__ - INFO - 139 - trainer - [0/257] Loss: 8.2820
2025/11/06 20:48:50 - __main__ - INFO - 139 - trainer - [52/257] Loss: 8.3839
2025/11/06 20:48:52 - __main__ - INFO - 139 - trainer - [104/257] Loss: 8.1676
2025/11/06 20:48:55 - __main__ - INFO - 139 - trainer - [156/257] Loss: 8.2003
2025/11/06 20:48:58 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.2941
2025/11/06 20:49:01 - __main__ - INFO - 104 - trainer - Epoch: 15
2025/11/06 20:49:01 - __main__ - INFO - 139 - trainer - [0/257] Loss: 8.2148
2025/11/06 20:49:04 - __main__ - INFO - 139 - trainer - [52/257] Loss: 8.2567
2025/11/06 20:49:06 - __main__ - INFO - 139 - trainer - [104/257] Loss: 8.0740
2025/11/06 20:49:09 - __main__ - INFO - 139 - trainer - [156/257] Loss: 8.1364
2025/11/06 20:49:12 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.0207
2025/11/06 20:49:15 - __main__ - INFO - 104 - trainer - Epoch: 16
2025/11/06 20:49:15 - __main__ - INFO - 139 - trainer - [0/257] Loss: 7.9989
2025/11/06 20:49:17 - __main__ - INFO - 139 - trainer - [52/257] Loss: 8.2343
2025/11/06 20:49:20 - __main__ - INFO - 139 - trainer - [104/257] Loss: 7.9896
2025/11/06 20:49:23 - __main__ - INFO - 139 - trainer - [156/257] Loss: 8.1302
2025/11/06 20:49:26 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.1249
2025/11/06 20:49:29 - __main__ - INFO - 104 - trainer - Epoch: 17
2025/11/06 20:49:29 - __main__ - INFO - 139 - trainer - [0/257] Loss: 7.9657
2025/11/06 20:49:31 - __main__ - INFO - 139 - trainer - [52/257] Loss: 8.2462
2025/11/06 20:49:34 - __main__ - INFO - 139 - trainer - [104/257] Loss: 8.0940
2025/11/06 20:49:37 - __main__ - INFO - 139 - trainer - [156/257] Loss: 8.1406
2025/11/06 20:49:40 - __main__ - INFO - 139 - trainer - [208/257] Loss: 8.0684
2025/11/06 20:49:43 - __main__ - INFO - 104 - trainer - Epoch: 18
2025/11/06 20:49:43 - __main__ - INFO - 139 - trainer - [0/257] Loss: 8.0110
2025/11/06 20:49:45 - __main__ - INFO - 139 - trainer - [52/257] Loss: 7.8448
2025/11/06 20:49:48 - __main__ - INFO - 139 - trainer - [104/257] Loss: 8.0643
