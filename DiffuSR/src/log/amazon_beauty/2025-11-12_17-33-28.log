2025/11/12 17:33:28 - __main__ - INFO - 115 - main - Namespace(dataset='amazon_beauty', log_file='log/', random_seed=1997, max_len=50, device=device(type='cuda', index=0), num_gpu=1, batch_size=512, hidden_size=128, dropout=0.1, emb_dropout=0.3, hidden_act='gelu', num_blocks=4, epochs=3, decay_step=100, gamma=0.1, metric_ks=[5, 10, 20], optimizer='Adam', lr=0.001, loss_lambda=0, weight_decay=0, momentum=None, schedule_sampler_name='lossaware', diffusion_steps=32, lambda_uncertainty=0.001, noise_schedule='trunc_lin', rescale_timesteps=True, eval_interval=20, patience=5, description='Diffu_norm_score', long_head=False, diversity_measure=False, epoch_time_avg=False, model='freqdiffusion', maxlen=200, hidden_units=50, num_epochs=1000, num_heads=1, dropout_rate=0.2, l2_emb=0.0, inference_only=False, state_dict_path=None, norm_first=False)
2025/11/12 17:33:29 - __main__ - INFO - 107 - trainer - Epoch: 0
2025/11/12 17:33:30 - __main__ - INFO - 243 - trainer - [0/256] Loss: 10.2505
2025/11/12 17:33:30 - __main__ - INFO - 245 - trainer - [0/256] Loss: 0.0000
2025/11/12 17:33:39 - __main__ - INFO - 243 - trainer - [52/256] Loss: 9.8609
2025/11/12 17:33:39 - __main__ - INFO - 245 - trainer - [52/256] Loss: 0.0000
2025/11/12 17:33:48 - __main__ - INFO - 243 - trainer - [104/256] Loss: 9.7197
2025/11/12 17:33:48 - __main__ - INFO - 245 - trainer - [104/256] Loss: 0.0000
2025/11/12 17:33:57 - __main__ - INFO - 243 - trainer - [156/256] Loss: 9.5621
2025/11/12 17:33:57 - __main__ - INFO - 245 - trainer - [156/256] Loss: 0.0000
2025/11/12 17:34:05 - __main__ - INFO - 243 - trainer - [208/256] Loss: 9.4314
2025/11/12 17:34:05 - __main__ - INFO - 245 - trainer - [208/256] Loss: 0.0000
2025/11/12 17:34:13 - __main__ - INFO - 107 - trainer - Epoch: 1
2025/11/12 17:34:14 - __main__ - INFO - 243 - trainer - [0/256] Loss: 9.1773
2025/11/12 17:34:14 - __main__ - INFO - 245 - trainer - [0/256] Loss: 0.0000
2025/11/12 17:34:23 - __main__ - INFO - 243 - trainer - [52/256] Loss: 9.1950
2025/11/12 17:34:23 - __main__ - INFO - 245 - trainer - [52/256] Loss: 0.0000
2025/11/12 17:34:31 - __main__ - INFO - 243 - trainer - [104/256] Loss: 9.1955
2025/11/12 17:34:31 - __main__ - INFO - 245 - trainer - [104/256] Loss: 0.0000
2025/11/12 17:34:40 - __main__ - INFO - 243 - trainer - [156/256] Loss: 9.1455
2025/11/12 17:34:40 - __main__ - INFO - 245 - trainer - [156/256] Loss: 0.0000
2025/11/12 17:34:49 - __main__ - INFO - 243 - trainer - [208/256] Loss: 9.1164
2025/11/12 17:34:49 - __main__ - INFO - 245 - trainer - [208/256] Loss: 0.0000
2025/11/12 17:34:57 - __main__ - INFO - 107 - trainer - Epoch: 2
2025/11/12 17:34:57 - __main__ - INFO - 243 - trainer - [0/256] Loss: 8.9221
2025/11/12 17:34:57 - __main__ - INFO - 245 - trainer - [0/256] Loss: 0.3296
2025/11/12 17:35:06 - __main__ - INFO - 243 - trainer - [52/256] Loss: 9.3749
2025/11/12 17:35:06 - __main__ - INFO - 245 - trainer - [52/256] Loss: 0.5415
2025/11/12 17:35:15 - __main__ - INFO - 243 - trainer - [104/256] Loss: 9.4441
2025/11/12 17:35:15 - __main__ - INFO - 245 - trainer - [104/256] Loss: 0.4554
2025/11/12 17:35:24 - __main__ - INFO - 243 - trainer - [156/256] Loss: 9.5591
2025/11/12 17:35:24 - __main__ - INFO - 245 - trainer - [156/256] Loss: 0.3672
2025/11/12 17:35:32 - __main__ - INFO - 243 - trainer - [208/256] Loss: 9.4125
2025/11/12 17:35:32 - __main__ - INFO - 245 - trainer - [208/256] Loss: 0.2529
2025/11/12 17:35:40 - __main__ - INFO - 323 - trainer - {'Best_HR@5': 0, 'Best_NDCG@5': 0, 'Best_HR@10': 0, 'Best_NDCG@10': 0, 'Best_HR@20': 0, 'Best_NDCG@20': 0}
2025/11/12 17:35:40 - __main__ - INFO - 324 - trainer - {'Best_epoch_HR@5': 0, 'Best_epoch_NDCG@5': 0, 'Best_epoch_HR@10': 0, 'Best_epoch_NDCG@10': 0, 'Best_epoch_HR@20': 0, 'Best_epoch_NDCG@20': 0}
